# MultiModal-UIDetection
A multi-modal UI control detection model based on YOLO and LLM-generated textual descriptions.

In this repository, you can find information, results, data, and source codes of the multi-modal user interface control detection model. The method and experiments are described and discussed in the paper titled "Multi-modal user interface control detection using cross-attention".

<h2>Multi-modal user interface control detection model</h2>
<p>The following image illustrates the overal architecture of the model:</p>
<br>
<img width="400" src="https://github.com/mmoradi-iut/MultiModal-UIDetection/blob/main/Figure-6.jpg">
